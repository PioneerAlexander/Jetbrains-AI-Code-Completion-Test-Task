{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers==4.46.1 in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (4.46.1)\n",
            "Requirement already satisfied: nltk in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (3.9.1)\n",
            "Requirement already satisfied: evaluate in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.4.3)\n",
            "Requirement already satisfied: Levenshtein in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.26.1)\n",
            "Requirement already satisfied: sacrebleu in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (2.4.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from transformers==4.46.1->-r requirements.txt (line 1)) (0.26.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers==4.46.1->-r requirements.txt (line 1)) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from transformers==4.46.1->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from transformers==4.46.1->-r requirements.txt (line 1)) (24.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from transformers==4.46.1->-r requirements.txt (line 1)) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from transformers==4.46.1->-r requirements.txt (line 1)) (0.20.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from transformers==4.46.1->-r requirements.txt (line 1)) (0.4.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from transformers==4.46.1->-r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers==4.46.1->-r requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from transformers==4.46.1->-r requirements.txt (line 1)) (3.14.0)\n",
            "Requirement already satisfied: joblib in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: click in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from nltk->-r requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 3)) (2024.3.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 3)) (2.19.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 3)) (0.70.16)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from Levenshtein->-r requirements.txt (line 4)) (3.10.1)\n",
            "Requirement already satisfied: lxml in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from sacrebleu->-r requirements.txt (line 5)) (5.3.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from sacrebleu->-r requirements.txt (line 5)) (0.9.0)\n",
            "Requirement already satisfied: colorama in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from sacrebleu->-r requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: portalocker in /home/kariakinaleksandr/.local/lib/python3.9/site-packages (from sacrebleu->-r requirements.txt (line 5)) (2.10.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 3)) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 3)) (3.9.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 3)) (16.0.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 3)) (6.0.5)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 3)) (1.9.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate->-r requirements.txt (line 3)) (23.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.1->-r requirements.txt (line 1)) (4.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.46.1->-r requirements.txt (line 1)) (2024.2.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.46.1->-r requirements.txt (line 1)) (2.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.46.1->-r requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.46.1->-r requirements.txt (line 1)) (3.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from pandas->evaluate->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.9/site-packages (from pandas->evaluate->-r requirements.txt (line 3)) (2024.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas->evaluate->-r requirements.txt (line 3)) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->evaluate->-r requirements.txt (line 3)) (1.16.0)\n",
            "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 24.3.1 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODuWazofcnR4"
      },
      "source": [
        "Import all necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zJqTT5encjZV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaKwiYP4gkCI"
      },
      "source": [
        "As repository, I decided to choose a solution of a test task about code completion I did 6 months ago. You can access this repository [here](https://github.com/PioneerAlexander/Improving-LLMs-on-underrepresented-programming-languages?tab=readme-ov-file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HB92Fxrld5UJ",
        "outputId": "9e514735-c7b8-408a-d180-a53774930fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-02 17:45:06--  https://github.com/PioneerAlexander/Improving-LLMs-on-underrepresented-programming-languages/archive/refs/heads/main.zip\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/PioneerAlexander/Improving-LLMs-on-underrepresented-programming-languages/zip/refs/heads/main [following]\n",
            "--2024-11-02 17:45:06--  https://codeload.github.com/PioneerAlexander/Improving-LLMs-on-underrepresented-programming-languages/zip/refs/heads/main\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.121.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.121.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘main.zip’\n",
            "\n",
            "main.zip                [ <=>                ] 479.79K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-11-02 17:45:06 (12.4 MB/s) - ‘main.zip’ saved [491300]\n",
            "\n",
            "Archive:  main.zip\n",
            "07a62a853673ea397c045c848c7028c757fd13d4\n",
            "   creating: Improving-LLMs-on-underrepresented-programming-languages-main/\n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/README.md  \n",
            "   creating: Improving-LLMs-on-underrepresented-programming-languages-main/configs/\n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/configs/finetune_phi-1_5.yaml  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/kt_filenames.txt  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/requirements.txt  \n",
            "   creating: Improving-LLMs-on-underrepresented-programming-languages-main/src/\n",
            " extracting: Improving-LLMs-on-underrepresented-programming-languages-main/src/__init__.py  \n",
            "   creating: Improving-LLMs-on-underrepresented-programming-languages-main/src/dataset/\n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/src/dataset/CodeXGLUETestDataset.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/src/dataset/FinetuningDataset.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/src/dataset/KotlinCodeCompletionDataset.py  \n",
            " extracting: Improving-LLMs-on-underrepresented-programming-languages-main/src/dataset/__init__.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/src/dataset/preprocess.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/src/dataset/train_test_dataset_split.py  \n",
            "   creating: Improving-LLMs-on-underrepresented-programming-languages-main/src/model/\n",
            " extracting: Improving-LLMs-on-underrepresented-programming-languages-main/src/model/__init__.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/src/model/eval_model.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/src/model/finetune_model_peft.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/src/model/metrics.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/src/model/save_phi-1_5_pretrained.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/src/model/utils.py  \n",
            "   creating: Improving-LLMs-on-underrepresented-programming-languages-main/src/parser/\n",
            " extracting: Improving-LLMs-on-underrepresented-programming-languages-main/src/parser/__init__.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/src/parser/parser.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/src/parser/utils.py  \n",
            "   creating: Improving-LLMs-on-underrepresented-programming-languages-main/test/\n",
            " extracting: Improving-LLMs-on-underrepresented-programming-languages-main/test/__init__.py  \n",
            "   creating: Improving-LLMs-on-underrepresented-programming-languages-main/test/dataset/\n",
            " extracting: Improving-LLMs-on-underrepresented-programming-languages-main/test/dataset/__init__.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/test/dataset/test_dataset.py  \n",
            "  inflating: Improving-LLMs-on-underrepresented-programming-languages-main/test/dataset/test_preprocess.py  \n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/PioneerAlexander/Improving-LLMs-on-underrepresented-programming-languages/archive/refs/heads/main.zip\n",
        "!unzip main.zip\n",
        "\n",
        "%mv Improving-LLMs-on-underrepresented-programming-languages-main repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess the code before splitting it into three parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R0jg1caf3HD",
        "outputId": "00d8dc44-bcaa-42b9-f61e-9e4c12975430"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted: repo/README.md\n",
            "Deleted: repo/requirements.txt\n",
            "Deleted: repo/kt_filenames.txt\n",
            "Deleted: repo/src/__init__.py\n",
            "Skipped: repo/src/parser/utils.py\n",
            "Skipped: repo/src/parser/parser.py\n",
            "Deleted: repo/src/parser/__init__.py\n",
            "Skipped: repo/src/dataset/train_test_dataset_split.py\n",
            "Skipped: repo/src/dataset/KotlinCodeCompletionDataset.py\n",
            "Skipped: repo/src/dataset/preprocess.py\n",
            "Skipped: repo/src/dataset/CodeXGLUETestDataset.py\n",
            "Deleted: repo/src/dataset/__init__.py\n",
            "Skipped: repo/src/dataset/FinetuningDataset.py\n",
            "Skipped: repo/src/model/save_phi-1_5_pretrained.py\n",
            "Skipped: repo/src/model/utils.py\n",
            "Skipped: repo/src/model/eval_model.py\n",
            "Skipped: repo/src/model/metrics.py\n",
            "Skipped: repo/src/model/finetune_model_peft.py\n",
            "Deleted: repo/src/model/__init__.py\n",
            "Deleted: repo/configs/finetune_phi-1_5.yaml\n",
            "Deleted: repo/test/__init__.py\n",
            "Skipped: repo/test/dataset/test_preprocess.py\n",
            "Skipped: repo/test/dataset/test_dataset.py\n",
            "Deleted: repo/test/dataset/__init__.py\n",
            "Moved: repo/src/parser/utils.py -> repo/\n",
            "Moved: repo/src/parser/parser.py -> repo/\n",
            "Moved: repo/src/dataset/train_test_dataset_split.py -> repo/\n",
            "Moved: repo/src/dataset/KotlinCodeCompletionDataset.py -> repo/\n",
            "Moved: repo/src/dataset/preprocess.py -> repo/\n",
            "Moved: repo/src/dataset/CodeXGLUETestDataset.py -> repo/\n",
            "Moved: repo/src/dataset/FinetuningDataset.py -> repo/\n",
            "Moved: repo/src/model/save_phi-1_5_pretrained.py -> repo/\n",
            "Moved: repo/src/model/utils.py -> repo/\n",
            "Moved: repo/src/model/eval_model.py -> repo/\n",
            "Moved: repo/src/model/metrics.py -> repo/\n",
            "Moved: repo/src/model/finetune_model_peft.py -> repo/\n",
            "Deleted empty directory: repo/src/parser\n",
            "Deleted empty directory: repo/src/dataset\n",
            "Deleted empty directory: repo/src/model\n",
            "Moved: repo/test/dataset/test_preprocess.py -> repo/\n",
            "Moved: repo/test/dataset/test_dataset.py -> repo/\n",
            "Deleted empty directory: repo/test/dataset\n",
            "Deleted empty directory: repo/src\n",
            "Deleted empty directory: repo/configs\n",
            "Deleted empty directory: repo/test\n"
          ]
        }
      ],
      "source": [
        "from code_preprocess_utils import delete_non_py_files, move_files_and_cleanup\n",
        "\n",
        "directory_path = 'repo/'\n",
        "\n",
        "delete_non_py_files(directory_path) # keep only python non-empty files\n",
        "move_files_and_cleanup(directory_path) # move files to the root directory and delete empty directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of lines from file to be used as context\n",
        "CONTEXT_LENGTH = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7oEspzaHlV2E"
      },
      "outputs": [],
      "source": [
        "from split_python_files import split_python_files\n",
        "\n",
        "corpus = split_python_files(directory_path, num_splits=4, context_length=CONTEXT_LENGTH)\n",
        "\n",
        "with open('corpus.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(corpus, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collected corpus length: 48\n"
          ]
        }
      ],
      "source": [
        "print(f\"Collected corpus length: {len(corpus)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = \"codellama/CodeLlama-7b-hf\"\n",
        "device = \"cuda\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model codellama/CodeLlama-7b-hf special_tokens: ['<s>', '</s>', '<unk>', '▁<PRE>', '▁<MID>', '▁<SUF>', '▁<EOT>']\n"
          ]
        }
      ],
      "source": [
        "special_tokens = tokenizer.all_special_tokens\n",
        "\n",
        "print(f\"Model {checkpoint} special_tokens: {special_tokens}\")\n",
        "\n",
        "\n",
        "prefix_token = special_tokens[3]\n",
        "middle_token = special_tokens[4]\n",
        "suffix_token = special_tokens[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_model_completions(corpus):\n",
        "    \"\"\"\n",
        "    Generate model completions for Fill-In-Middle taks\n",
        "    \"\"\"\n",
        "    model_completions = []\n",
        "\n",
        "    for code in tqdm(corpus):\t\n",
        "        prompt = f\"{prefix_token} {code['prefix']} {suffix_token} {code['suffix']} {middle_token}\"\n",
        "        \n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        \n",
        "        desired_tokens_size = len(inputs[\"input_ids\"][0]) + len(tokenizer.encode(code[\"middle\"], return_tensors=\"pt\")[0])\n",
        "           \n",
        "        outputs = model.generate(**inputs, max_length=desired_tokens_size)\n",
        "        model_completions.append(tokenizer.decode(outputs[0], skip_special_tokens=False))\n",
        "\t\n",
        "    return model_completions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_completions = generate_model_completions(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "expected_completions = [code[\"middle\"] for code in corpus]\n",
        "\n",
        "pairs = [{\"model_completion\": completion.split(middle_token[1:])[-1], \"expected_completion\": code} for completion, code in zip(\n",
        "    model_completions, expected_completions)]\n",
        "\n",
        "with open(\"pairs.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(pairs, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pairs.json file contains the model completions and the expected completions for the Fill-In-Middle task. I evaluated each model response by score from 0 to 5, where 0 is the worst score and 5 is the best score, and added this score to the pairs.json file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from metrics import bleu_score, exact_match, edit_similarity, chrf_score\n",
        "\n",
        "my_judgement_scores = []\n",
        "bleu_scores = []\n",
        "exact_match_scores = []\n",
        "edit_similarity_scores = []\n",
        "chrf_scores = []\n",
        "\n",
        "with open(\"pairs.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    pairs = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/48 [00:00<?, ?it/s]/home/kariakinaleksandr/.local/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/home/kariakinaleksandr/.local/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            " 12%|█▎        | 6/48 [00:14<01:42,  2.44s/it]/home/kariakinaleksandr/.local/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 48/48 [02:07<00:00,  2.66s/it]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for pair in tqdm(pairs):\n",
        "    expected_completion = pair[\"expected_completion\"]\n",
        "    model_completion = pair[\"model_completion\"]\n",
        "\n",
        "    bleu_scores.append(bleu_score(expected_completion, model_completion))\n",
        "    exact_match_scores.append(exact_match(expected_completion, model_completion)[\"exact_match\"])\n",
        "    edit_similarity_scores.append(edit_similarity(expected_completion, model_completion))\n",
        "    chrf_scores.append(chrf_score(expected_completion, model_completion)[\"score\"])\n",
        "    my_judgement_scores.append(pair[\"score\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exact_match_scores [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Correlation between my judgement and BLEU score: 0.5823924405943129\n",
            "Correlation between my judgement and Exact Match: nan\n",
            "Correlation between my judgement and Edit Similarity: 0.7643977832029185\n",
            "Correlation between my judgement and CHRF score: 0.8524125158512138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
            "  c /= stddev[None, :]\n"
          ]
        }
      ],
      "source": [
        "my_judgement_scores = np.array(my_judgement_scores)\n",
        "bleu_scores = np.array(bleu_scores)\n",
        "exact_match_scores = np.array(exact_match_scores)\n",
        "edit_similarity_scores = np.array(edit_similarity_scores)\n",
        "\n",
        "print(\"exact_match_scores\", exact_match_scores)\n",
        "\n",
        "print(f\"Correlation between my judgement and BLEU score: {np.corrcoef(my_judgement_scores, bleu_scores)[0, 1]}\")\n",
        "print(f\"Correlation between my judgement and Exact Match: {np.corrcoef(my_judgement_scores, exact_match_scores)[0, 1]}\")\n",
        "print(f\"Correlation between my judgement and Edit Similarity: {np.corrcoef(my_judgement_scores, edit_similarity_scores)[0, 1]}\")\n",
        "print(f\"Correlation between my judgement and CHRF score: {np.corrcoef(my_judgement_scores, chrf_scores)[0, 1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of course, the correlation between anything and vector of zeros is not defined (resulting in nan). Exact match score is 0 for all pairs, because even when the model generates in its answer the same code as the expected completion, there was a difference by a new line symbol in the start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "for pair in pairs:\n",
        "    pair[\"bleu_score\"] = bleu_scores[pairs.index(pair)]\n",
        "    pair[\"exact_match_score\"] = exact_match_scores[pairs.index(pair)]\n",
        "    pair[\"edit_similarity_score\"] = edit_similarity_scores[pairs.index(pair)]\n",
        "    pair[\"chrf_score\"] = chrf_scores[pairs.index(pair)]\n",
        "\n",
        "with open(\"resulting_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(pairs, f, ensure_ascii=False, indent=4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
